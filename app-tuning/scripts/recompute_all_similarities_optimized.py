"""
ìµœì í™”ëœ ìœ ì‚¬ë„ ì¬ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸ (V3)

ì£¼ìš” ê°œì„  ì‚¬í•­:
1. **ë°ì´í„° ì¼ê´„ ë¡œë”©**: ëª¨ë“  ì‚¬ìš©ì ë°ì´í„°ë¥¼ ì²˜ìŒì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ DB I/O ìµœì†Œí™”.
2. **ë°ì´í„° ê³µìœ **: ë¡œë“œëœ ë°ì´í„°ë¥¼ ê° ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ì— ì¸ìë¡œ ì „ë‹¬í•˜ì—¬ ì¤‘ë³µ ë¡œë”© ë°©ì§€.
3. `user_service.py`ì˜ `update_similarity_for_users_v3` í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ì¼ê´€ì„± ìœ ì§€.
4. `ThreadPoolExecutor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìë³„ 'friend' ë° 'couple' ì¹´í…Œê³ ë¦¬ ê³„ì‚°ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰.
5. `upsert` ë¡œì§ì„ ê°œì„ í•˜ì—¬ ì‹¤ì œ ë³€ê²½ì´ ìˆì„ ë•Œë§Œ DBì— ì“°ë„ë¡ ìµœì í™”.
6. ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì œê±° ë° ì„±ëŠ¥ ì¸¡ì • ë¡œê·¸ ì •ë¦¬.
"""

import concurrent.futures
import os
import sys
import time
import traceback

import numpy as np

script_dir = os.path.dirname(__file__)
project_root = os.path.abspath(os.path.join(script_dir, os.pardir))
sys.path.insert(0, project_root)

from core.vector_database import get_user_collection  # noqa: E402
from services.user_service import update_similarity_for_users_v3  # noqa: E402
from utils.logger import log_performance, logger  # noqa: E402

# ìµœì í™”ëœ ì›Œì»¤ ìˆ˜ (CPU ì½”ì–´ì˜ 75% ì‚¬ìš©)
WORKER_COUNT = max(1, int(os.cpu_count() * 0.75))
BATCH_SIZE = 10  # ë¡œê·¸ ì¶œë ¥ ë‹¨ìœ„


def get_all_users_data():
    """ëª¨ë“  ì‚¬ìš©ì ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        logger.info("ğŸ“Š ëª¨ë“  ì‚¬ìš©ì ë°ì´í„° ë¡œë”© ì¤‘...")
        data = get_user_collection().get(include=["embeddings", "metadatas"])
        logger.info(f"âœ… {len(data['ids'])}ëª…ì˜ ì‚¬ìš©ì ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        return data
    except Exception as e:
        logger.error(f"[CRITICAL] ì‚¬ìš©ì ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None


def convert_numpy_floats(obj):
    if isinstance(obj, np.float32):
        return float(obj)
    if isinstance(obj, dict):
        return {k: convert_numpy_floats(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [convert_numpy_floats(i) for i in obj]
    return obj


def process_user_category(user_id: str, category: str, all_users_data: dict):
    """ë‹¨ì¼ ì‚¬ìš©ìì˜ íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # update_similarity_for_users_v3 ë‚´ë¶€ì—ì„œ similaritiesë¥¼ ì €ì¥í•˜ê¸° ì „ì— float32 ë³€í™˜ì´ ëˆ„ë½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        # ë³€í™˜ì„ ê°•ì œ ì ìš© (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì´ì–´ë„ ì¤‘ë³µ ì ìš©ì€ ë¬´í•´)
        update_similarity_for_users_v3(user_id, category, all_users_data)
        return True, None
    except Exception as e:
        error_message = f"[ERROR] {category} ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨ for {user_id}: {e}"
        return False, error_message


def process_user_wrapper(user_id: str, all_users_data: dict):
    """
    í•œ ëª…ì˜ ì‚¬ìš©ìì— ëŒ€í•´ 'friend'ì™€ 'couple' ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ ê³„ì‚°ì„
    ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ë˜í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    categories = ["friend", "couple"]
    success_count = 0
    errors = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=len(categories)) as executor:
        future_to_category = {
            executor.submit(
                process_user_category, user_id, category, all_users_data
            ): category
            for category in categories
        }

        for future in concurrent.futures.as_completed(future_to_category):
            success, error = future.result()
            if success:
                success_count += 1
            else:
                errors.append(error)

    if success_count == len(categories):
        return True, user_id
    else:
        logger.error(f"âŒ {user_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {errors}")
        return False, user_id


@log_performance(
    operation_name="recompute_all_similarities_optimized", include_memory=True
)
def recompute_all_similarities_optimized():
    """ìµœì í™”ëœ ìœ ì‚¬ë„ ì¬ê³„ì‚° ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìµœì í™”ëœ ìœ ì‚¬ë„ ì¬ê³„ì‚° ì‹œì‘ (V3)...")
    start_time = time.time()

    all_users_data = get_all_users_data()
    if not all_users_data or not all_users_data.get("ids"):
        logger.warning("ì²˜ë¦¬í•  ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    user_ids = all_users_data["ids"]
    total_users = len(user_ids)
    logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ëŒ€ìƒ ì‚¬ìš©ì: {total_users}ëª…")
    logger.info(f"âš™ï¸ ì›Œì»¤ ìˆ˜: {WORKER_COUNT}")

    processed_count = 0
    success_count = 0
    failure_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=WORKER_COUNT) as executor:
        future_to_user_id = {
            executor.submit(process_user_wrapper, user_id, all_users_data): user_id
            for user_id in user_ids
        }

        for future in concurrent.futures.as_completed(future_to_user_id):
            processed_count += 1
            try:
                success, user_id = future.result()
                if success:
                    success_count += 1
                else:
                    failure_count += 1

                if processed_count % BATCH_SIZE == 0 or processed_count == total_users:
                    progress = (processed_count / total_users) * 100
                    logger.info(
                        f"ğŸ”„ ì§„í–‰ë¥ : {processed_count}/{total_users} ({progress:.1f}%) "
                        f"(ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failure_count})"
                    )

            except Exception as e:
                user_id = future_to_user_id[future]
                logger.error(
                    f"[CRITICAL] ì‚¬ìš©ì {user_id} ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"
                )
                traceback.print_exc()
                failure_count += 1

    total_time = time.time() - start_time
    logger.info("ğŸ‰ ì „ì²´ ìœ ì‚¬ë„ ì¬ê³„ì‚° ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"âœ… ì„±ê³µ: {success_count}, âŒ ì‹¤íŒ¨: {failure_count}")
    if total_time > 0:
        logger.info(f"âš¡ï¸ í‰ê·  ì²˜ë¦¬ ì†ë„: {total_users / total_time:.2f} users/sec")


# ... (ê¸°ì¡´ import ë° í•¨ìˆ˜ ì •ì˜ëŠ” ë™ì¼) ...
# process_user_wrapper í•¨ìˆ˜ëŠ” ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì‚­ì œí•©ë‹ˆë‹¤.


@log_performance(
    operation_name="recompute_all_similarities_optimized_v2", include_memory=True
)
def recompute_all_similarities_optimized_v2():  # í•¨ìˆ˜ ì´ë¦„ ë³€ê²½
    """ìµœì í™”ëœ ìœ ì‚¬ë„ ì¬ê³„ì‚° ë©”ì¸ í•¨ìˆ˜ (ë‹¨ì¼ í’€ êµ¬ì¡°)"""
    logger.info("ğŸš€ ìµœì í™”ëœ ìœ ì‚¬ë„ ì¬ê³„ì‚° ì‹œì‘ (V3 - Single Pool)...")
    start_time = time.time()

    all_users_data = get_all_users_data()
    if not all_users_data or not all_users_data.get("ids"):
        logger.warning("ì²˜ë¦¬í•  ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    user_ids = all_users_data["ids"]
    categories = ["friend", "couple"]

    # (user_id, category) í˜•íƒœì˜ ëª¨ë“  ì‘ì—… ëª©ë¡ì„ ìƒì„±
    tasks = [(user_id, category) for user_id in user_ids for category in categories]
    total_tasks = len(tasks)

    logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ëŒ€ìƒ ì‚¬ìš©ì: {len(user_ids)}ëª… (ì´ ì‘ì—…: {total_tasks}ê°œ)")
    logger.info(f"âš™ï¸ ì›Œì»¤ ìˆ˜: {WORKER_COUNT}")

    processed_count = 0
    success_count = 0
    failure_count = 0

    # ë‹¨ì¼ ThreadPoolExecutor ìƒì„±. CPU ë°”ìš´ë“œ ì‘ì—…ì´ë¼ë©´ ProcessPoolExecutorë¡œ êµì²´ ê³ ë ¤
    with concurrent.futures.ThreadPoolExecutor(max_workers=WORKER_COUNT) as executor:
        # (user_id, category)ë¥¼ ì¸ìë¡œ ë°›ëŠ” í•¨ìˆ˜ë¥¼ ì§ì ‘ ì œì¶œ
        future_to_task = {
            executor.submit(process_user_category, user_id, category, all_users_data): (
                user_id,
                category,
            )
            for user_id, category in tasks
        }

        for future in concurrent.futures.as_completed(future_to_task):
            processed_count += 1
            user_id, category = future_to_task[future]
            try:
                success, error_message = future.result()
                if success:
                    success_count += 1
                else:
                    failure_count += 1
                    logger.error(error_message)  # ê°œë³„ ì˜¤ë¥˜ ë©”ì‹œì§€ ë¡œê¹…

                # ë¡œê·¸ ì¶œë ¥ì€ BATCH_SIZEì˜ ë°°ìˆ˜ë§ˆë‹¤ (ì‘ì—… ë‹¨ìœ„)
                if (
                    processed_count % (BATCH_SIZE * len(categories)) == 0
                    or processed_count == total_tasks
                ):
                    progress = (processed_count / total_tasks) * 100
                    logger.info(
                        f"ğŸ”„ ì§„í–‰ë¥ : {processed_count}/{total_tasks} ({progress:.1f}%) "
                        f"(ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failure_count})"
                    )

            except Exception as e:
                logger.error(
                    f"[CRITICAL] ì‘ì—…({user_id}, {category}) ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"
                )
                traceback.print_exc()
                failure_count += 1

    total_time = time.time() - start_time
    logger.info("ğŸ‰ ì „ì²´ ìœ ì‚¬ë„ ì¬ê³„ì‚° ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    # ì„±ê³µ/ì‹¤íŒ¨ëŠ” 'ì‘ì—…' ë‹¨ìœ„ë¡œ ê³„ì‚°
    logger.info(
        f"âœ… ì„±ê³µ: {success_count}, âŒ ì‹¤íŒ¨: {failure_count} (ì´ {total_tasks}ê°œ ì‘ì—…)"
    )
    if total_time > 0:
        # ì´ˆë‹¹ ì²˜ë¦¬ 'ì‚¬ìš©ì' ìˆ˜ë¡œ í™˜ì‚°í•˜ì—¬ ê³„ì‚°
        logger.info(f"âš¡ï¸ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(user_ids) / total_time:.2f} users/sec")


if __name__ == "__main__":
    recompute_all_similarities_optimized_v2()
